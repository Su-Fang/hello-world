---
title: "Madissoon et al Tissue stability_lung"
output:
  html_document:
    theme: Tissue stability_lung 2020/05/17
    df_print: kable
date: 'Compiled: `r format(Sys.Date(), "%B %d, %Y")`'
---
***

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  tidy = TRUE,
  tidy.opts = list(width.cutoff = 120),
  message = FALSE,
  warning = FALSE
)
```

### Setup the Seurat Object ########
* Load the .rds data downloaded from  (https://www.tissuestabilitycellatlas.org/)

###### Start from loading rds file" ######
*** directly load processed data ~ 1 Gb)
```{r}
lung <- readRDS(file = "lung_ts.rds")

lung
```



```{r}
levels(lung@meta.data$Celltypes)

```


```{r fig.height=11, fig.width=9}
cd_genes <- c("IL4", "IFNG","STAT3", "RNASE7", "RNASE1", "CTSL","TMPRSS2","ACE2")
DotPlot(object = lung, features = cd_genes, cols = c("royalblue", "red3"), group.by = "Celltypes")+ RotatedAxis()

```

# Dot plots - the size of the dot corresponds to the percentage of cells expressing the feature,
in each cluster. The color represents the average expression level
```{r fig.height=11, fig.width=9}
features <- c("IL10", "AREG", "IL6", "IL13", "IL5", "IL4", "FN1",
              "TNF", "IL1B", "IL18", "TGFBR2", "TSLP", "IL33")

DotPlot(lung, group.by = "Celltypes", features = features,
        cols = c("blue", "red"), 
        dot.min = 0, dot.scale = 6) + RotatedAxis()  # Cool!

```


# Dot plots - repeat Ziegler Figure 4B
```{r}
features <- c("RNASE7", "TMPRSS2", "ACE2", "CDHR3", "JAK1", "LYPD2", "MUC5AC", "PIFO","CAPS",
              "AZGP1", "LTF", "AQP3", "SERPINB3", "TP63", "KRT5")

DotPlot(PolyAll, group.by = "seurat_clusters", features = features,
        cols = c("gray90", "grey10"), col.min = -2.5, col.max = 2.5,
        dot.min = 0, dot.scale = 6) + RotatedAxis()

```

```{r}
features1 <- c("IL17A", "STAT3", "RNASE7", "RNASE6","RNASE4", "RNASE2","RNASE1","TMPRSS2","ACE2")

DotPlot(PolyAll, group.by = "subset", features = features1,
        cols = c("royalblue", "red2"), col.min = -2.5, col.max = 2.5,
        dot.min = 0, dot.scale = 6) + RotatedAxis()
```


```{r}
features1 <- c("INFG", "STAT3", "RNASE7", "RNASE6","RNASE4", "RNASE2","RNASE1","TMPRSS2","ACE2")

DotPlot(PolyAll, group.by = "polyp", features = features1,
        cols = c("royalblue", "red2"), col.min = -2.5, col.max = 2.5,
        dot.min = 0, dot.scale = 6) + RotatedAxis()
```

        




```{r init}
PolyAll.data =
  as.sparse(read.table("20180822_PolypAll_cleaned_data.txt", header=TRUE, row.names = 1))

```

```{r}
class(PolyAll.data) #196 Mb
dim(PolyAll.data)
```

* Load the metadata
```{r}
PolyAll.metadata = 
  read.table("20180822_PolypAll_cleaned_metadata.txt", header=TRUE, row.names = 1)
```

```{r}
dim(PolyAll.metadata)
colnames(PolyAll.metadata)
```


**What does data in a count matrix look like?**
Lets examine a few genes in the first thirty cellsThe `.` values in the matrix represent 0s (no molecules detected). Since most values in an scRNA-seq matrix are 0,  Seurat uses a sparse-matrix representation whenever possible. 

```{r}
PolyAll.data[c("KRT5","RNASE7","MS4A1"), 1:30] # succeeded in sparse!!!^o^
```


```{r}
dense.size <- object.size(x = as.matrix(x = PolyAll.data))
dense.size
sparse.size <- object.size(x = PolyAll.data)
sparse.size
dense.size / sparse.size
```

```{r}
head(PolyAll.data)
```


```{r}
# Initialize the Seurat object with the raw (non-normalized data).
PolyAll <- CreateSeuratObject(counts = PolyAll.data, project = "Polyp_ALL_2018", min.cells = 3, min.features = 200, meta.data = PolyAll.metadata)
```

```{r}
PolyAll #Large Seyrat (379.7 Mb)
```


```{r}
head(x = PolyAll@meta.data, 5) #succeeded

```


###### Standard pre-processing workflow ######

The steps below encompass the standard pre-processing workflow for scRNA-seq data in Seurat. These represent *** *** 1. the selection and filtration of cells based on QC metrics
*** 2. data normalization and scaling
*** 3. the detection of highly variable features

*** 1. QC and selecting cells for further analysis


```{r mito, fig.height=7, fig.width=13}
# The [[ operator can add columns to object metadata. This is a great place to stash QC stats
PolyAll[["percent.mt"]] <- PercentageFeatureSet(object = PolyAll, pattern = "^MT-")

```

**Where are QC metrics stored in Seurat? The number of unique genes and total molecules are automatically calculated during `CreateSeuratObject` You can find them stored in the object meta data

```{r qc, fig.height=7, fig.width=13}
# Show QC metrics for the first 5 cells
head(x = PolyAll@meta.data, 5)
```

In the example below, we visualize QC metrics, and use these to filter cells.

* We filter cells that have unique feature counts over 2,500 or less than 200
* We filter cells that have >5% mitochondrial counts
    
```{r qc2, fig.height=7, fig.width=13}

#Visualize QC metrics as a violin plot
VlnPlot(object = PolyAll, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3)

# FeatureScatter is typically used to visualize feature-feature relationships, but can be used for anything calculated by the object, i.e. columns in object metadata, PC scores etc.

plot1 <- FeatureScatter(object = PolyAll, feature1 = "nCount_RNA", feature2 = "percent.mt") 
plot2 <- FeatureScatter(object = PolyAll, feature1 = "nCount_RNA", feature2 = "nFeature_RNA") 
CombinePlots(plots = list(plot1,plot2))

```


```{r}
PolyAll <- subset(x = PolyAll, 
           subset = nFeature_RNA > 200 & nFeature_RNA < 4000 & percent.mt < 5)
```

```{r}
PolyAll # remove 18036-17983 = 53 samples!
```

*** 2. data normalization and scaling = Normalizing the data

After removing unwanted cells from the dataset, the next step is to normalize the data. By default, we employ a global-scaling normalization method "LogNormalize" that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result. Normalized values are stored in `PolyAll[["RNA"]]@data`.

```{r normalize}
PolyAll <- NormalizeData(object = PolyAll, normalization.method = "LogNormalize", scale.factor = 1e4)
```
For clarity, in this previous line of code (and in future commands), we provide the default values for certain parameters in the function call. However, this isn't required and the same behavior can be achieved with:

```{r normalize.default, eval = FALSE}
PolyAll <- NormalizeData(object = PolyAll)
```

*** 3. the detection of highly variable features = Identification of highly variable features (feature selection)

We next calculate a subset of features that exhibit high cell-to-cell variation in the dataset (i.e, they are highly expressed in some cells, and lowly expressed in others). We and [others](https://www.nature.com/articles/nmeth.2645) have found that focusing on these genes in downstream analysis helps to highlight biological signal in single-cell datasets.

Our procedure in Seurat3 is described in detail [here](https://www.biorxiv.org/content/early/2018/11/02/460147.full.pdf), and improves on previous versions by directly modeling the mean-variance relationship inherent in single-cell data, and is implemented in the `FindVariableFeatures` function. By default, we return 2,000 features per dataset. These will be used in downstream analysis, like PCA.

```{r var_features, fig.height=5, fig.width=11}
PolyAll <- FindVariableFeatures(object = PolyAll,selection.method = 'vst', nfeatures = 2000)

# Identify the 10 most highly variable genes
top10 <- head(x = VariableFeatures(object = PolyAll), 10)

# plot variable features with and without labels
plot1 <- VariableFeaturePlot(object = PolyAll)
plot2 <- LabelPoints(plot = plot1, points = top10, repel = TRUE)
CombinePlots(plots = list(plot1, plot2))
```

*** Scaling the data

Next, we apply a linear transformation ('scaling') that is a standard pre-processing step prior to dimensional reduction techniques like PCA. The `ScaleData` function:

* Shifts the expression of each gene, so that the mean expression across cells is 0
* Scales the expression of each gene, so that the variance across cells is 1
    + This step gives equal weight in downstream analyses, so that highly-expressed genes do not dominate
* The results of this are stored in `PolyAll[["RNA"]]@scale.data`
**This step takes too long! Can I make it faster?** and size `becomes 10X bigger ---> 3.4 Gb`!!

```{r regress, fig.height=7, fig.width=11, results='hide'}
all.genes <- rownames(x = PolyAll)
PolyAll <- ScaleData(object = PolyAll, features = all.genes)
```

Scaling is an essential step in the Seurat workflow, but only on genes that will be used as input to PCA. Therefore, the default in `ScaleData` is only to perform scaling on the previously identified variable features (2,000 by default). To do this, omit the `features` argument in the previous function call, i.e.

```{r regressvar, fig.height=7, fig.width=11, results='hide',eval = FALSE}
PolyAll <- ScaleData(object = PolyAll)
```
Your PCA and clustering results will be unaffected. However, Seurat heatmaps (produced as shown below with `DoHeatmap`) require genes in the heatmap to be scaled, to make sure highly-expressed genes don't dominate the heatmap. To make sure we don't leave any genes out of the heatmap later, we are scaling all genes in this tutorial. 

**How can I remove unwanted sources of variation, as in Seurat v2?**

In `Seurat v2` we also use the `ScaleData` function to remove unwanted sources of variation from a single-cell dataset. For example, we could 'regress out' heterogeneity associated with (for example) cell cycle stage, or mitochondrial contamination. These features are still supported in `ScaleData` in `Seurat v3`, i.e.: 

```{r regressvarmt, fig.height=7, fig.width=11, results='hide',eval = FALSE}
PolyAll <- ScaleData(object = PolyAll, vars.to.regress = 'percent.mt') ## Great, the size is back to 1/5 (655.9 Mb)
```
However, particularly for advanced users who would like to use this functionality, we strongly recommend the use of our new normalization workflow, `sctransform`. The method is described in our recent [preprint](https://www.biorxiv.org/content/10.1101/576827v2), with a separate vignette using Seurat v3 [here](https://www.dropbox.com/s/r84d0a4b4mihltf/sctransform_vignette.pdf). As with `ScaleData`, the function `SCTransform` also includes a `vars.to.regress` parameter.

*** Perform linear dimensional reduction

Next we perform PCA on the scaled data. By default, only the previously determined variable features are used as input, but can be defined using `features` argument if you wish to choose a different subset.

```{r pca,results='hide'}
PolyAll <- RunPCA(object = PolyAll, features = VariableFeatures(object = PolyAll))
```


Seurat provides several useful ways of visualizing both cells and features that define the PCA, including `VizDimReduction`, `DimPlot`, and `DimHeatmap`

```{r pca_viz, message=TRUE}
# Examine and visualize PCA results a few different ways
print(x = PolyAll[['pca']], dims = 1:5, nfeatures = 5)
VizDimLoadings(object = PolyAll, dims = 1:2, reduction = 'pca')
DimPlot(object = PolyAll, reduction = 'pca')
```

In particular `DimHeatmap` allows for easy exploration of the primary sources of heterogeneity in a dataset, and can be useful when trying to decide which PCs to include for further downstream analyses. Both cells and features are ordered according to their PCA scores. Setting `cells` to a number plots the 'extreme' cells on both ends of the spectrum, which dramatically speeds plotting for large datasets. Though clearly a supervised analysis, we find this to be a valuable tool for exploring correlated feature sets.

```{r single-heatmap}
DimHeatmap(object = PolyAll, dims = 1, cells = 500, balanced = TRUE)
```

```{r multi-heatmap, fig.height=12, fig.width=9}
DimHeatmap(object = PolyAll, dims = 1:15, cells = 500, balanced = TRUE)
```

***Determine the 'dimensionality' of the dataset

To overcome the extensive technical noise in any single feature for scRNA-seq data, Seurat clusters cells based on their PCA scores, with each PC essentially representing a 'metafeature' that combines information across a correlated feature set. The top principal components therefore represent a robust compression of the dataset. However, how many componenets should we choose to include? 10? 20? 100?

In [Macosko *et al*](http://www.cell.com/abstract/S0092-8674(15)00549-8), we implemented a resampling test inspired by the JackStraw procedure. We randomly permute a subset of the data (1% by default) and rerun PCA, constructing a 'null distribution' of feature scores, and repeat this procedure. We identify 'significant' PCs as those who have a strong enrichment of low p-value features.

```{r jackstraw, fig.height=6, fig.width=10}
# NOTE: This process can take a long time for big datasets, comment out for expediency. More approximate techniques such as those implemented in ElbowPlot() can be used to reduce computation time
PolyAll <- JackStraw(object = PolyAll, num.replicate = 100)
PolyAll <- ScoreJackStraw(object = PolyAll, dims = 1:20)
```

The `JackStrawPlot` function provides a visualization tool for comparing the distribution of p-values for each PC with a uniform distribution (dashed line). 'Significant' PCs will show a strong enrichment of features with low p-values (solid curve above the dashed line). In this case it appears that there is a sharp drop-off in significance after the first 10-12 PCs.

```{r jsplots, fig.height=6, fig.width=10}
JackStrawPlot(object = PolyAll, dims = 1:15)
```

An alternative heuristic method generates an 'Elbow plot': a ranking of principle components based on the percentage of variance explained by each one (`ElbowPlot` function). In this example, we can observe an 'elbow' around PC9-10, suggesting that the majority of true signal is captured in the first 10 PCs. 

```{r elbow_plot, fig.height=6, fig.width=10}
ElbowPlot(object = PolyAll)
```

Identifying the true dimensionality of a dataset -- can be challenging/uncertain for the user. We therefore suggest these three approaches to consider. The first is more supervised, exploring PCs to determine relevant sources of heterogeneity, and could be used in conjunction with GSEA for example. The second implements a statistical test based on a random null model, but is time-consuming for large datasets, and may not return a clear PC cutoff. The third is a heuristic that is commonly used, and can be calculated instantly. In this example, all three approaches yielded similar results, but we might have been justified in choosing anything between PC 7-12 as a cutoff. 

We chose 15 here, but encourage users to consider the following:

*** Cluster the cells

Seurat v3 applies a graph-based clustering approach, building upon initial strategies in ([Macosko *et al*](http://www.cell.com/abstract/S0092-8674(15)00549-8)). Importantly, the *distance metric* which drives the clustering analysis (based on previously identified PCs) remains the same. However, our approach to partioning the cellular distance matrix into clusters has dramatically improved. Our approach was heavily inspired by recent manuscripts which applied graph-based clustering approaches to scRNA-seq data [[SNN-Cliq, Xu and Su, Bioinformatics, 2015]](http://bioinformatics.oxfordjournals.org/content/early/2015/02/10/bioinformatics.btv088.abstract) and CyTOF data [[PhenoGraph, Levine *et al*., Cell, 2015]](http://www.ncbi.nlm.nih.gov/pubmed/26095251). Briefly, these methods embed cells in a graph structure - for example a K-nearest neighbor (KNN) graph, with edges drawn between cells with similar feature expression patterns, and then attempt to partition this graph into highly interconnected 'quasi-cliques' or 'communities'. 

As in PhenoGraph, we first construct a KNN graph based on the euclidean distance in PCA space, and refine the edge weights between any two cells based on the shared overlap in their local neighborhoods (Jaccard similarity). This step is performed using the `FindNeighbors` function, and takes as input the previously defined dimensionality of the dataset (first 10 PCs).

To cluster the cells, we next apply modularity optimization techniques such as the Louvain algorithm (default) or SLM [[SLM, Blondel *et al*., Journal of Statistical Mechanics]](http://dx.doi.org/10.1088/1742-5468/2008/10/P10008), to iteratively group cells together, with the goal of optimizing the standard modularity function. The `FindClusters` function implements this procedure, and contains a resolution parameter that sets the 'granularity' of the downstream clustering, with increased values leading to a greater number of clusters. We find that setting this parameter between 0.4-1.2 typically returns good results for single-cell datasets of around 3K cells. Optimal resolution often increases for larger datasets. The clusters can be found using the `Idents` function.


```{r cluster, fig.height=5, fig.width=7}
PolyAll <- FindNeighbors(object = PolyAll, dims = 1:15)
PolyAll <- FindClusters(object = PolyAll, resolution = 0.5)

# Look at cluster IDs of the first 5 cells
head(x = Idents(object = PolyAll), 5)
```

*** Run non-linear dimensional reduction (UMAP/tSNE)

Seurat offers several non-linear dimensional reduction techniques, such as tSNE and UMAP, to visualize and explore these datasets. The goal of these algorithms is to learn the underlying manifold of the data in order to place similar cells together in low-dimensional space. Cells within the graph-based clusters determined above should co-localize on these dimension reduction plots. As input to the UMAP and tSNE, we suggest using the same PCs as input to the clustering analysis.

```{r UMAP, fig.height=5, fig.width=7}
# If you haven't installed UMAP, you can do so via reticulate::py_install(packages = "umap-learn")
PolyAll <- RunUMAP(object = PolyAll, dims = 1:10)
```


```{r tSNE, fig.height=5, fig.width=7}

PolyAll <- RunTSNE(object = PolyAll, dims = 1:10)


```



```{r tsneplot, fig.height=5, fig.width=7}
# note that you can set `label = TRUE` or use the LabelClusters function to help label individual clusters
DimPlot(object = PolyAll, reduction = 'umap')
```


```{r}
DimPlot(object = PolyAll, reduction = 'tsne', group.by = 'subset')
```


PolyAll@
```{r}
head(PolyAll$polyp)
DimPlot(object = PolyAll, reduction = 'umap', group.by = 'polyp')

```


#### Finding differentially expressed features (cluster biomarkers) ####

Seurat can help you find markers that define clusters via differential expression. By default, it identifes positive and negative markers of a single cluster (specified in `ident.1`), compared to all other cells.  `FindAllMarkers` automates this process for all clusters, but you can also test groups of clusters vs. each other, or against all cells.

The `min.pct` argument requires a feature to be detected at a minimum percentage in either of the two groups of cells, and the thresh.test argument requires a feature to be differentially expressed (on average) by some amount between the two groups. You can set both of these to 0, but with a dramatic increase in time - since this will test a large number of features that are unlikely to be highly discriminatory. As another option to speed up these computations, `max.cells.per.ident` can be set. This will downsample each identity class to have no more cells than whatever this is set to. While there is generally going to be a loss in power, the speed increases can be significiant and the most highly differentially expressed features will likely still rise to the top.

```{r markers1, fig.height=8, fig.width=15}
# find all markers of cluster 1
cluster1.markers <- FindMarkers(object = PolyAll, ident.1 = 1, min.pct = 0.25)
head(x = cluster1.markers, n = 5)
# find all markers distinguishing cluster 5 from clusters 0 and 3
cluster5.markers <- FindMarkers(object = PolyAll, ident.1 = 5, ident.2 = c(0, 3), min.pct = 0.25)
head(x = cluster5.markers, n = 5)
# find markers for every cluster compared to all remaining cells, report only the positive ones
PolyAll.markers <- FindAllMarkers(object = PolyAll, only.pos = TRUE, min.pct = 0.25, logfc.threshold = 0.25)
PolyAll.markers %>% group_by(cluster) %>% top_n(n = 2, wt = avg_logFC)
```

Seurat has several tests for differential expression which can be set with the test.use parameter (see our [DE vignette](http://satijalab01.nygenome.org/seurat/v3.0/de_vignette.html) for details). For example, the ROC test returns the 'classification power' for any individual marker (ranging from 0 - random, to 1 - perfect).

```{r markersroc, fig.height=8, fig.width=15}
cluster1.markers <- FindMarkers(object = PolyAll, ident.1 = 0, logfc.threshold = 0.25, test.use = "roc", only.pos = TRUE)
```

We include several tools for visualizing marker expression. `VlnPlot` (shows expression probability distributions across clusters), and `FeaturePlot` (visualizes feature expression on a tSNE or PCA plot) are our most commonly used visualizations. We also suggest exploring `RidgePlot`, `CellScatter`, and `DotPlot` as additional methods to view your dataset.

```{r markerplots, fig.height=8, fig.width=15}
VlnPlot(object = PolyAll, features = c("MS4A1", "CD79A", "DDR1", "CTSS"))
# you can plot raw counts as well
VlnPlot(object = PolyAll, features = c("NKG7", "PF4", "DDR1", "DDR2"), slot = 'counts', log = TRUE)
FeaturePlot(object = PolyAll, features = c("MS4A1", "GNLY", "CD3E", "CD14", "FCER1A", "FCGR3A", "LYZ", "PPBP", "CD8A", "CTSS"))
```

`DoHeatmap` generates an expression heatmap for given cells and features. In this case, we are plotting the top 20 markers (or all markers if less than 20) for each cluster.

```{r clusterHeatmap, fig.height=8, fig.width=15}
PolyAll.markers %>% group_by(cluster) %>% top_n(n = 10, wt = avg_logFC) -> top10
DoHeatmap(object = PolyAll, features = top10$gene) + NoLegend()
```



```{r labelplot, fig.height=5, fig.width=9}

DimPlot(object = PolyAll, reduction = 'umap', label = TRUE, pt.size = 0.5, group.by = "subset") + NoLegend()
```

```{r save.img, include=FALSE}
library(ggplot2)
plot <- DimPlot(object = PolyAll, reduction = "umap", label = TRUE, label.size = 4.5, 
                group.by = "subset") + xlab("UMAP 1") + ylab("UMAP 2") + 
  theme(axis.title = element_text(size = 18), legend.text = element_text(size = 18)) + 
  guides(colour = guide_legend(override.aes = list(size = 10)))
ggsave(filename = "PolyAll_0516_umap.png", height = 7, width = 12, plot = plot)
```


You can save the object at this point so that it can easily be loaded back in without having to rerun the computationally intensive steps performed above, or easily shared with collaborators.

```{r saveobject, eval=FALSE}
saveRDS(PolyAll, file = "PolyAll_0516.rds")
```




```{r}
head(PolyAll@meta.data$subset)
```


* Pre-filter features that are detected at <50% frequency in either CD14+ Monocytes or FCGR3A+ Monocytes


```{r}
head(FindMarkers(PolyAll, ident.1 = "CD14+ Mono", ident.2 = "FCGR3A+ Mono", min.pct = 0.5))
```




* Pre-filter features that have less than a two-fold change between the average expression of CD14+ Monocytes vs FCGR3A+ Monocytes


```{r}
head(FindMarkers(PolyAll, ident.1 = "CD14+ Mono", ident.2 = "FCGR3A+ Mono", logfc.threshold = log(2)))
```


######## New data visualization methods in v3.0  Compiled: 2020-04-17  tinyurl.com/yapwpq9y ########

```{r}
class(PolyAll)
names(PolyAll)
PolyAll
dim(PolyAll)
```

```{r}
class(PolyAll_small) # # [1] "Seurat" attr(,"package") [1] "Seurat"
dim(PolyAll_small) #[1] 230  80
head(PolyAll_small)
```


```{r plots, fig.height=8, fig.width=15}
VlnPlot(object = PolyAll, features = c("ACE2","TMPRSS2","RNASE7", "CD14", "CD8A"), group.by = "polyp")
```


* you can plot raw counts as well

```{r}
VlnPlot(object = PolyAll, group.by = "subset", features = c("ACE2", "TMPRSS2", "RNASE7"), 
        slot = 'counts', log = TRUE)
```

```{r}
FeaturePlot(object = PolyAll, 
            features = c("IL33", "TSLP", "TGFB2", "ACE2", "TMPRSS2", "RNASE1", "RNASE7"))
```




# find all markers distinguishing 'Basal' from others

```{r}
basal.markers <- FindMarkers(object = PolyAll, ident.1 = "Basal",
                group.by = 'subset', min.pct = 0.25)
head(x = basal.markers, n = 20) # succeed!!
```


```{r}
head(PolyAll@meta.data$polyp)
```


# find all markers distinguishing 'Yes' from 'others 'No' for @polyp


```{r}
polypYES.markers <- FindMarkers(object = PolyAll, ident.1 = "YES",
                group.by = 'polyp', min.pct = 0.25)
head(x = polypYES.markers, n = 20) # succeed!!
```



* Ridge plots - from ggridges. Visualize single cell expression distributions in each cluster
```{r}
features <- c("LYZ",  "ACE2", "RNASE7")
RidgePlot(PolyAll, features = features, ncol = 3, group.by = "subset")
```


* Violin plot - Visualize single cell expression distributions in each cluster
```{r}
features <- c("TMPRSS2",  "ACE2", "RNASE7", "RNASE8", "RNASE9", "RNASE10")
VlnPlot(PolyAll, features = features, group.by = "subset")
```


*Feature plot - visualize feature expression in low-dimensional space

```{r}
FeaturePlot(PolyAll, features = features)
```

* Single cell heatmap of feature expression

```{r}
DoHeatmap(subset(PolyAll, downsample = 100), features = features, size = 3)
```


        
```{r}
#session_info 2020/05/17 ----------
devtools::session_info()
```
        














